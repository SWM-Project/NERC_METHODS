{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Copy of bi_lstm_crf_conll2003_FINAL.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AtO26COvqJZ",
        "outputId": "ace736eb-cc35-4382-86e5-d195b7adde70"
      },
      "source": [
        "### THIS IS THE FINAL NOTEBOOK\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = \"/content/drive/Shareddrives/SWM/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyVCOclnqbm9",
        "outputId": "6d328b6c-91e2-4d56-f9e3-2dc80949e267"
      },
      "source": [
        "# from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "\n",
        "import time\n",
        "import _pickle as cPickle\n",
        "\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 80\n",
        "plt.style.use('seaborn-pastel')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np\n",
        "print(\"Import Successful.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import Successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-8qsrEqbnA",
        "outputId": "06d81dda-4489-42e3-c1cf-16f6d313d719"
      },
      "source": [
        "#parameters for the Model\n",
        "def set_params():\n",
        "    parameters = OrderedDict()\n",
        "    parameters['train'] = data_dir + \"data/eng.train\" #Path to train file\n",
        "    parameters['dev'] = data_dir + \"data/eng.testa\" #Path to test file\n",
        "    parameters['test'] = data_dir + \"data/eng.testb\" #Path to dev file\n",
        "    parameters['tag_scheme'] = \"iob\" #BIO or BIOES\n",
        "    parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
        "    parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
        "    parameters['char_dim'] = 30 #Char embedding dimension\n",
        "    parameters['word_dim'] = 50 #Token embedding dimension\n",
        "    parameters['word_lstm_dim'] = 100 #Token LSTM hidden layer size\n",
        "    parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
        "    parameters['embedding_path'] = data_dir + \"glove.6B/glove.6B.50d.txt\" #Location of pretrained embeddings\n",
        "    parameters['all_emb'] = 1 #Load all embeddings\n",
        "    parameters['crf'] =1 #Use CRF (0 to disable)\n",
        "    parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
        "    parameters['epoch'] =  25 #Number of epochs to run\"\n",
        "    parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
        "    parameters['name'] = \"self-trained-model\" # Model name\n",
        "    parameters['gradient_clip']=5.0\n",
        "    parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
        "    parameters['reload'] = \"models/pre-trained-model\" \n",
        "    return parameters\n",
        "\n",
        "#Constants\n",
        "parameters = set_params()\n",
        "use_gpu = parameters['use_gpu']\n",
        "START_TAG = '<START>'\n",
        "STOP_TAG = '<STOP>'\n",
        "models_path = \"models/\"\n",
        "mapping_file = data_dir + 'data/mapping.pkl'\n",
        "name = parameters['name']\n",
        "model_name = models_path + name\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "\n",
        "print(mapping_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/SWM/data/mapping.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGLzTJkIqbnC"
      },
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace every digit in a string by a zero.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path, zeros):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in codecs.open(path, 'r', 'utf8'):\n",
        "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
        "        if not line:\n",
        "            if len(sentence) > 0:\n",
        "                if 'DOCSTART' not in sentence[0][0]:\n",
        "                    sentences.append(sentence)\n",
        "                sentence = []\n",
        "        else:\n",
        "            word = line.split()\n",
        "            assert len(word) >= 2\n",
        "            sentence.append(word)\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAE35rJuqbnC"
      },
      "source": [
        "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
        "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
        "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB84XO-dqbnC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "cf257351-66c4-4646-c77d-7a7c0eb40b62"
      },
      "source": [
        "def iob2(tags):\n",
        "    \"\"\"\n",
        "    Check that tags have a valid BIO format.\n",
        "    Tags in BIO1 format are converted to BIO2.\n",
        "    \"\"\"\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            continue\n",
        "        split = tag.split('-')\n",
        "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
        "            return False\n",
        "        if split[0] == 'B':\n",
        "            continue\n",
        "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "        elif tags[i - 1][1:] == tag[1:]:\n",
        "            continue\n",
        "        else:  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "    return True\n",
        "\n",
        "def iob_iobes(tags):\n",
        "    \"\"\"\n",
        "    the function is used to convert\n",
        "    BIO -> BIOES tagging\n",
        "    \"\"\"\n",
        "    new_tags = []\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            new_tags.append(tag)\n",
        "        elif tag.split('-')[0] == 'B':\n",
        "            if i + 1 != len(tags) and \\\n",
        "               tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('B-', 'S-'))\n",
        "        elif tag.split('-')[0] == 'I':\n",
        "            if i + 1 < len(tags) and \\\n",
        "                    tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('I-', 'E-'))\n",
        "        else:\n",
        "            raise Exception('Invalid IOB format!')\n",
        "    return new_tags\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "    \"\"\"\n",
        "    Check and update sentences tagging scheme to IOB2.\n",
        "    Only IOB1 and IOB2 schemes are accepted.\n",
        "    \"\"\"\n",
        "    for i, s in enumerate(sentences):\n",
        "        tags = [w[-1] for w in s]\n",
        "        # Check that tags are given in the IOB format\n",
        "        if not iob2(tags):\n",
        "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "            raise Exception('Sentences should be given in IOB format! ' +\n",
        "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
        "        print(tag_scheme)\n",
        "        if tag_scheme == 'iob':\n",
        "            # If format was IOB1, we convert to IOB2\n",
        "            for word, new_tag in zip(s, tags):\n",
        "                word[-1] = new_tag\n",
        "        elif tag_scheme == 'iobes':\n",
        "            new_tags = iob_iobes(tags)\n",
        "            for word, new_tag in zip(s, new_tags):\n",
        "                word[-1] = new_tag\n",
        "        else:\n",
        "            raise Exception('Unknown tagging scheme!')\n",
        "\n",
        "'''\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "    \"\"\"\n",
        "    Check and update sentences tagging scheme to BIO2\n",
        "    Only BIO1 and BIO2 schemes are accepted for input data.\n",
        "    \"\"\"\n",
        "    for i, s in enumerate(sentences):\n",
        "        tags = [w[-1] for w in s]\n",
        "        # Check that tags are given in the BIO format\n",
        "        if not iob2(tags):\n",
        "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "            raise Exception('Sentences should be given in BIO format! ' +\n",
        "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
        "        if tag_scheme == 'BIOES':\n",
        "            new_tags = iob_iobes(tags)\n",
        "            for word, new_tag in zip(s, new_tags):\n",
        "                word[-1] = new_tag\n",
        "        else:\n",
        "            raise Exception('Wrong tagging scheme!')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef update_tag_scheme(sentences, tag_scheme):\\n    \"\"\"\\n    Check and update sentences tagging scheme to BIO2\\n    Only BIO1 and BIO2 schemes are accepted for input data.\\n    \"\"\"\\n    for i, s in enumerate(sentences):\\n        tags = [w[-1] for w in s]\\n        # Check that tags are given in the BIO format\\n        if not iob2(tags):\\n            s_str = \\'\\n\\'.join(\\' \\'.join(w) for w in s)\\n            raise Exception(\\'Sentences should be given in BIO format! \\' +\\n                            \\'Please check sentence %i:\\n%s\\' % (i, s_str))\\n        if tag_scheme == \\'BIOES\\':\\n            new_tags = iob_iobes(tags)\\n            for word, new_tag in zip(s, new_tags):\\n                word[-1] = new_tag\\n        else:\\n            raise Exception(\\'Wrong tagging scheme!\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98RejQaGqbnD"
      },
      "source": [
        "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSXr_6PmqbnE"
      },
      "source": [
        "def create_dico(item_list):\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list\n",
        "    dico = {}\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico:\n",
        "                dico[item] = 1\n",
        "            else:\n",
        "                dico[item] += 1\n",
        "    return dico\n",
        "\n",
        "def create_mapping(dico):\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences, lower):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
        "    dico = create_dico(words)\n",
        "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words)\n",
        "    ))\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def char_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and mapping of characters, sorted by frequency.\n",
        "    \"\"\"\n",
        "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
        "    dico = create_dico(chars)\n",
        "    char_to_id, id_to_char = create_mapping(dico)\n",
        "    print(\"Found %i unique characters\" % len(dico))\n",
        "    return dico, char_to_id, id_to_char\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    dico[START_TAG] = -1\n",
        "    dico[STOP_TAG] = -2\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJhGpthjqbnE",
        "outputId": "60677adc-3552-45c0-f498-5c148a47d1a3"
      },
      "source": [
        "def pickle_me(name, obj):\n",
        "    file_path = data_dir + \"pickle2/\" + name + \".pkl\"\n",
        "    file_obj = open(file_path, 'wb')\n",
        "    cPickle.dump(obj, file_obj)\n",
        "    file_obj.close()\n",
        "    print(\"Pickled the obj {} at {}\".format(name, file_path))\n",
        "    return\n",
        "\n",
        "\n",
        "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
        "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
        "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
        "\n",
        "pickle_me('word_to_id', word_to_id)\n",
        "pickle_me('id_to_word', id_to_word)\n",
        "pickle_me('char_to_id', char_to_id)\n",
        "pickle_me('id_to_char', id_to_char)\n",
        "pickle_me('tag_to_id', tag_to_id)\n",
        "pickle_me('id_to_tag', id_to_tag)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17493 unique words (203621 in total)\n",
            "Found 75 unique characters\n",
            "Found 11 unique named entity tags\n",
            "Pickled the obj word_to_id at /content/drive/Shareddrives/SWM/pickle2/word_to_id.pkl\n",
            "Pickled the obj id_to_word at /content/drive/Shareddrives/SWM/pickle2/id_to_word.pkl\n",
            "Pickled the obj char_to_id at /content/drive/Shareddrives/SWM/pickle2/char_to_id.pkl\n",
            "Pickled the obj id_to_char at /content/drive/Shareddrives/SWM/pickle2/id_to_char.pkl\n",
            "Pickled the obj tag_to_id at /content/drive/Shareddrives/SWM/pickle2/tag_to_id.pkl\n",
            "Pickled the obj id_to_tag at /content/drive/Shareddrives/SWM/pickle2/id_to_tag.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssodt4mqbnG"
      },
      "source": [
        "def lower_case(x,lower=False):\n",
        "    if lower:\n",
        "        return x.lower()  \n",
        "    else:\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cLmC7-CqbnH",
        "outputId": "48dfbf0d-9af2-4823-9bf3-f4c6024c3f48"
      },
      "source": [
        "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
        "        - word indexes\n",
        "        - word char indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for s in sentences:\n",
        "        str_words = [w[0] for w in s]\n",
        "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
        "                 for w in str_words]\n",
        "        # Skip characters that are not in the training set\n",
        "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s]\n",
        "        data.append({\n",
        "            'str_words': str_words,\n",
        "            'words': words,\n",
        "            'chars': chars,\n",
        "            'tags': tags,\n",
        "        })\n",
        "    return data\n",
        "\n",
        "train_data = prepare_dataset(\n",
        "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "dev_data = prepare_dataset(\n",
        "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "test_data = prepare_dataset(\n",
        "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")\n",
        "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14041 / 3250 / 3453 sentences in train / dev / test.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5x5fJBrqbnI",
        "outputId": "c9ecfcbe-cb37-405f-cf92-72dffadb78c5"
      },
      "source": [
        "all_word_embeds = {}\n",
        "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
        "    s = line.strip().split()\n",
        "    if len(s) == parameters['word_dim'] + 1:\n",
        "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
        "\n",
        "#Intializing Word Embedding Matrix\n",
        "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
        "\n",
        "for w in word_to_id:\n",
        "    if w in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
        "    elif w.lower() in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
        "\n",
        "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 pretrained embeddings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdwK6i8gqbnI",
        "outputId": "6daa859f-b381-4ae8-b52d-78974cd7a637"
      },
      "source": [
        "with open(mapping_file, 'wb') as f:\n",
        "    mappings = {\n",
        "        'word_to_id': word_to_id,\n",
        "        'tag_to_id': tag_to_id,\n",
        "        'char_to_id': char_to_id,\n",
        "        'parameters': parameters,\n",
        "        'word_embeds': word_embeds\n",
        "    }\n",
        "    cPickle.dump(mappings, f)\n",
        "\n",
        "print('word_to_id: ', len(word_to_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_to_id:  17493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-sblEjCqbnJ"
      },
      "source": [
        "def init_embedding(input_embedding):\n",
        "    \"\"\"\n",
        "    Initialize embedding\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "    nn.init.uniform(input_embedding, -bias, bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol46ghHoqbnJ"
      },
      "source": [
        "def init_linear(input_linear):\n",
        "    \"\"\"\n",
        "    Initialize linear transformation\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
        "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
        "    if input_linear.bias is not None:\n",
        "        input_linear.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v24M6A8qbnK"
      },
      "source": [
        "def init_lstm(input_lstm):\n",
        "    \"\"\"\n",
        "    Initialize lstm\n",
        "    \n",
        "    PyTorch weights parameters:\n",
        "    \n",
        "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
        "            `(hidden_size * hidden_size)`\n",
        "            \n",
        "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * hidden_size)`            \n",
        "    \"\"\"\n",
        "    \n",
        "    # Weights init for forward layer\n",
        "    for ind in range(0, input_lstm.num_layers):\n",
        "        \n",
        "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
        "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
        "        \n",
        "        # Initialize the sampling range\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        \n",
        "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        # Similar to above but for the hidden-hidden weights of the current layer\n",
        "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        \n",
        "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
        "    if input_lstm.bidirectional:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "\n",
        "    # Bias initialization steps\n",
        "    \n",
        "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
        "    if input_lstm.bias:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
        "            \n",
        "            # Initializing to zero\n",
        "            bias.data.zero_()\n",
        "            \n",
        "            # This is the range of indices for our forget gates for each LSTM cell\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "            #Similar for the hidden-hidden layer\n",
        "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
        "            bias.data.zero_()\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
        "        if input_lstm.bidirectional:\n",
        "            for ind in range(0, input_lstm.num_layers):\n",
        "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCF96lKqbnL"
      },
      "source": [
        "def log_sum_exp(vec):\n",
        "    '''\n",
        "    This function calculates the score explained above for the forward algorithm\n",
        "    vec 2D: 1 * tagset_size\n",
        "    '''\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "    \n",
        "def argmax(vec):\n",
        "    '''\n",
        "    This function returns the max index in a vector\n",
        "    '''\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return to_scalar(idx)\n",
        "\n",
        "def to_scalar(var):\n",
        "    '''\n",
        "    Function to convert pytorch tensor to a scalar\n",
        "    '''\n",
        "    return var.view(-1).data.tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhnEvW-qbnL"
      },
      "source": [
        "def score_sentences(self, feats, tags):\n",
        "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
        "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
        "    r = torch.LongTensor(range(feats.size()[0]))\n",
        "    if self.use_gpu:\n",
        "        r = r.cuda()\n",
        "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "    else:\n",
        "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "\n",
        "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
        "\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oVtCVprqbnM"
      },
      "source": [
        "def forward_alg(self, feats):\n",
        "    '''\n",
        "    This function performs the forward algorithm explained above\n",
        "    '''\n",
        "    # calculate in log domain\n",
        "    # feats is len(sentence) * tagset_size\n",
        "    # initialize alpha with a Tensor with values all equal to -10000.\n",
        "    \n",
        "    # Do the forward algorithm to compute the partition function\n",
        "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    \n",
        "    # START_TAG has all of the score.\n",
        "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "    \n",
        "    # Wrap in a variable so that we will get automatic backprop\n",
        "    forward_var = autograd.Variable(init_alphas)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "        \n",
        "    # Iterate through the sentence\n",
        "    for feat in feats:\n",
        "        # broadcast the emission score: it is the same regardless of\n",
        "        # the previous tag\n",
        "        emit_score = feat.view(-1, 1)\n",
        "        \n",
        "        # the ith entry of trans_score is the score of transitioning to\n",
        "        # next_tag from i\n",
        "        tag_var = forward_var + self.transitions + emit_score\n",
        "        \n",
        "        # The ith entry of next_tag_var is the value for the\n",
        "        # edge (i -> next_tag) before we do log-sum-exp\n",
        "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
        "        \n",
        "        # The forward variable for this tag is log-sum-exp of all the\n",
        "        # scores.\n",
        "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
        "        \n",
        "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
        "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    # Z(x)\n",
        "    return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkMk4bOdqbnN"
      },
      "source": [
        "def viterbi_algo(self, feats):\n",
        "    '''\n",
        "    In this function, we implement the viterbi algorithm explained above.\n",
        "    A Dynamic programming based approach to find the best tag sequence\n",
        "    '''\n",
        "    backpointers = []\n",
        "    # analogous to forward\n",
        "    \n",
        "    # Initialize the viterbi variables in log space\n",
        "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "    \n",
        "    # forward_var at step i holds the viterbi variables for step i-1\n",
        "    forward_var = Variable(init_vvars)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "    for feat in feats:\n",
        "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
        "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
        "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
        "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
        "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
        "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
        "        if self.use_gpu:\n",
        "            viterbivars_t = viterbivars_t.cuda()\n",
        "            \n",
        "        # Now add in the emission scores, and assign forward_var to the set\n",
        "        # of viterbi variables we just computed\n",
        "        forward_var = viterbivars_t + feat\n",
        "        backpointers.append(bptrs_t)\n",
        "\n",
        "    # Transition to STOP_TAG\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
        "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
        "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
        "    path_score = terminal_var[best_tag_id]\n",
        "    \n",
        "    # Follow the back pointers to decode the best path.\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "        best_tag_id = bptrs_t[best_tag_id]\n",
        "        best_path.append(best_tag_id)\n",
        "        \n",
        "    # Pop off the start tag (we dont want to return that to the caller)\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXUDhkJIqbnN"
      },
      "source": [
        "def forward_calc(self, sentence, chars, chars2_length, d):\n",
        "    \n",
        "    '''\n",
        "    The function calls viterbi decode and generates the \n",
        "    most probable sequence of tags for the sentence\n",
        "    '''\n",
        "    \n",
        "    # Get the emission scores from the BiLSTM\n",
        "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
        "    # viterbi to get tag_seq\n",
        "    \n",
        "    # Find the best path, given the features.\n",
        "    if self.use_crf:\n",
        "        score, tag_seq = self.viterbi_decode(feats)\n",
        "    else:\n",
        "        score, tag_seq = torch.max(feats, 1)\n",
        "        tag_seq = list(tag_seq.cpu().data)\n",
        "\n",
        "    return score, tag_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMrVeSW8qbnO"
      },
      "source": [
        "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
        "    \n",
        "    chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
        "    \n",
        "    ## Creating Character level representation using Convolutional Neural Netowrk\n",
        "    ## followed by a Maxpooling Layer\n",
        "    chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
        "    chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
        "                                         kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
        "\n",
        "    ## Loading word embeddings\n",
        "    embeds = self.word_embeds(sentence)\n",
        "    \n",
        "    ## We concatenate the word embeddings and the character level representation\n",
        "    ## to create unified representation for each word\n",
        "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
        "\n",
        "    embeds = embeds.unsqueeze(1)\n",
        "    \n",
        "    ## Dropout on the unified embeddings\n",
        "    embeds = self.dropout(embeds)\n",
        "    \n",
        "    ## Word lstm\n",
        "    ## Takes words as input and generates a output at each step\n",
        "    lstm_out, _ = self.lstm(embeds)\n",
        "    \n",
        "    ## Reshaping the outputs from the lstm layer\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
        "    \n",
        "    ## Dropout on the lstm output\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "    \n",
        "    ## Linear layer converts the ouput vectors to tag space\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    \n",
        "    return lstm_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btuqgfnRqbnP"
      },
      "source": [
        "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
        "    # sentence, tags is a list of ints\n",
        "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
        "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
        "\n",
        "    if self.use_crf:\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "    else:\n",
        "        tags = Variable(tags)\n",
        "        scores = nn.functional.cross_entropy(feats, tags)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz8HPiL9qbnP"
      },
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
        "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
        "                 , use_crf=True):\n",
        "        '''\n",
        "        Input parameters:\n",
        "                \n",
        "                vocab_size= Size of vocabulary (int)\n",
        "                tag_to_ix = Dictionary that maps NER tags to indices\n",
        "                embedding_dim = Dimension of word embeddings (int)\n",
        "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
        "                char_to_ix = Dictionary that maps characters to indices\n",
        "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
        "                char_out_dimension = Output dimension from the CNN encoder for character\n",
        "                char_embedding_dim = Dimension of the character embeddings\n",
        "                use_gpu = defines availability of GPU, \n",
        "                    when True: CUDA function calls are made\n",
        "                    else: Normal CPU function calls are made\n",
        "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
        "        '''\n",
        "        \n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        \n",
        "        #parameter initialization for the model\n",
        "        self.use_gpu = use_gpu\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.use_crf = use_crf\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.out_channels = char_out_dimension\n",
        "\n",
        "        if char_embedding_dim is not None:\n",
        "            self.char_embedding_dim = char_embedding_dim\n",
        "            \n",
        "            #Initializing the character embedding layer\n",
        "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
        "            init_embedding(self.char_embeds.weight)\n",
        "            \n",
        "            #Performing CNN encoding on the character embeddings\n",
        "            self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
        "\n",
        "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if pre_word_embeds is not None:\n",
        "            #Initializes the word embeddings with pretrained word embeddings\n",
        "            self.pre_word_embeds = True\n",
        "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
        "        else:\n",
        "            self.pre_word_embeds = False\n",
        "    \n",
        "        #Initializing the dropout layer, with dropout specificed in parameters\n",
        "        self.dropout = nn.Dropout(parameters['dropout'])\n",
        "        \n",
        "        #Lstm Layer:\n",
        "        #input dimension: word embedding dimension + character level representation\n",
        "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
        "        \n",
        "        #Initializing the lstm layer using predefined function for initialization\n",
        "        init_lstm(self.lstm)\n",
        "        \n",
        "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "        \n",
        "        #Initializing the linear layer using predefined function for initialization\n",
        "        init_linear(self.hidden2tag) \n",
        "\n",
        "        if self.use_crf:\n",
        "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
        "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
        "            self.transitions = nn.Parameter(\n",
        "                torch.zeros(self.tagset_size, self.tagset_size))\n",
        "            \n",
        "            # These two statements enforce the constraint that we never transfer\n",
        "            # to the start tag and we never transfer from the stop tag\n",
        "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "    #assigning the functions, which we have defined earlier\n",
        "    _score_sentence = score_sentences\n",
        "    _get_lstm_features = get_lstm_features\n",
        "    _forward_alg = forward_alg\n",
        "    viterbi_decode = viterbi_algo\n",
        "    neg_log_likelihood = get_neg_log_likelihood\n",
        "    forward = forward_calc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-bnHns6qbnP",
        "outputId": "49732fc1-1223-4513-a1f6-3d3698fc3d9e"
      },
      "source": [
        "#creating the model using the Class defined above\n",
        "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
        "                   tag_to_ix=tag_to_id,\n",
        "                   embedding_dim=parameters['word_dim'],\n",
        "                   hidden_dim=parameters['word_lstm_dim'],\n",
        "                   use_gpu=use_gpu,\n",
        "                   char_to_ix=char_to_id,\n",
        "                   pre_word_embeds=word_embeds,\n",
        "                   use_crf=parameters['crf'])\n",
        "print(\"Model Initialized!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Initialized!!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-IOpT1UqbnP",
        "outputId": "6e6218b3-2ccf-49eb-fb56-74eca25e0aba"
      },
      "source": [
        "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
        "if parameters['reload']:\n",
        "    if not os.path.exists(parameters['reload']):\n",
        "      parameters['reload'] = False\n",
        "        # print(\"downloading pre-trained model\")\n",
        "        # model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
        "        # urllib.request.urlretrieve(model_url, parameters['reload'])\n",
        "    else:\n",
        "      model.load_state_dict(torch.load(parameters['reload']))\n",
        "    print(\"model reloaded :\", parameters['reload'])\n",
        "\n",
        "if use_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model reloaded : False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFHXlezoqbnQ"
      },
      "source": [
        "#Initializing the optimizer\n",
        "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
        "#learning rate=0.015 and momentum=0.9 \n",
        "#decay_rate=0.05 \n",
        "\n",
        "learning_rate = 0.015\n",
        "momentum = 0.9\n",
        "number_of_epochs = parameters['epoch'] \n",
        "decay_rate = 0.05\n",
        "gradient_clip = parameters['gradient_clip']\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "#variables which will used in training process\n",
        "losses = [] #list to store all losses\n",
        "loss = 0.0 #Loss Initializatoin\n",
        "best_dev_F = -1.0 # Current best F-1 Score on Dev Set\n",
        "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
        "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
        "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
        "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
        "plot_every = 2000 # Store loss after this many iterations\n",
        "count = 0 #Counts the number of iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK6PIVHvqbnR"
      },
      "source": [
        "def get_chunk_type(tok, idx_to_tag):\n",
        "    \"\"\"\n",
        "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
        "    as defined in BIOES\n",
        "    \n",
        "    Args:\n",
        "        tok: id of token, ex 4\n",
        "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
        "\n",
        "    Returns:\n",
        "        tuple: \"B\", \"PER\"\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    tag_name = idx_to_tag[tok]\n",
        "    tag_class = tag_name.split('-')[0]\n",
        "    tag_type = tag_name.split('-')[-1]\n",
        "    return tag_class, tag_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kZ9D1BCqbnR"
      },
      "source": [
        "def get_chunks(seq, tags):\n",
        "    \"\"\"Given a sequence of tags, group entities and their position\n",
        "\n",
        "    Args:\n",
        "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
        "        tags: dict[\"O\"] = 4\n",
        "\n",
        "    Returns:\n",
        "        list of (chunk_type, chunk_start, chunk_end)\n",
        "\n",
        "    Example:\n",
        "        seq = [4, 5, 0, 3]\n",
        "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
        "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We assume by default the tags lie outside a named entity\n",
        "    default = tags[\"O\"]\n",
        "    \n",
        "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "    \n",
        "    chunks = []\n",
        "    \n",
        "    chunk_type, chunk_start = None, None\n",
        "    for i, tok in enumerate(seq):\n",
        "        # End of a chunk 1\n",
        "        if tok == default and chunk_type is not None:\n",
        "            # Add a chunk.\n",
        "            chunk = (chunk_type, chunk_start, i)\n",
        "            chunks.append(chunk)\n",
        "            chunk_type, chunk_start = None, None\n",
        "\n",
        "        # End of a chunk + start of a chunk!\n",
        "        elif tok != default:\n",
        "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
        "            if chunk_type is None:\n",
        "                # Initialize chunk for each entity\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
        "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
        "                # or, if the chunk type is different from the previous one, then we\n",
        "                # start labelling it as a new entity\n",
        "                chunk = (chunk_type, chunk_start, i)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # end condition\n",
        "    if chunk_type is not None:\n",
        "        chunk = (chunk_type, chunk_start, len(seq))\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFwmRKCGqbnR"
      },
      "source": [
        "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
        "    '''\n",
        "    The function takes as input the model, data and calcuates F-1 Score\n",
        "    It performs conditional updates \n",
        "     1) Flag to save the model \n",
        "     2) Best F-1 score\n",
        "    ,if the F-1 score calculated improves on the previous F-1 score\n",
        "    '''\n",
        "    # Initializations\n",
        "    prediction = [] # A list that stores predicted tags\n",
        "    save = False # Flag that tells us if the model needs to be saved\n",
        "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
        "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
        "    \n",
        "    for data in datas:\n",
        "        ground_truth_id = data['tags']\n",
        "        words = data['str_words']\n",
        "        chars2 = data['chars']\n",
        "\n",
        "        d = {} \n",
        "        \n",
        "        # Padding the each word to max word size of that sentence\n",
        "        chars2_length = [len(c) for c in chars2]\n",
        "        char_maxl = max(chars2_length)\n",
        "        chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "        for i, c in enumerate(chars2):\n",
        "            chars2_mask[i, :chars2_length[i]] = c\n",
        "        chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "        dwords = Variable(torch.LongTensor(data['words']))\n",
        "        \n",
        "        # We are getting the predicted output from our model\n",
        "        if use_gpu:\n",
        "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "        else:\n",
        "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
        "        predicted_id = out\n",
        "    \n",
        "        \n",
        "        # We use the get chunks function defined above to get the true chunks\n",
        "        # and the predicted chunks from true labels and predicted labels respectively\n",
        "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
        "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
        "                                         tag_to_id))\n",
        "\n",
        "        # Updating the count variables\n",
        "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
        "        total_preds   += len(lab_pred_chunks)\n",
        "        total_correct += len(lab_chunks)\n",
        "    \n",
        "    # Calculating the F1-Score\n",
        "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
        "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
        "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
        "\n",
        "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
        "    \n",
        "    # If our current F1-Score is better than the previous best, we update the best\n",
        "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
        "    \n",
        "    if new_F>best_F:\n",
        "        best_F=new_F\n",
        "        save=True\n",
        "\n",
        "    return best_F, new_F, save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgkP2hdvqbnS"
      },
      "source": [
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"\n",
        "    shrink learning rate\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HaXyqzUKqbnT",
        "outputId": "3cc4a98b-bf2a-48a3-d746-96db2def0ab6"
      },
      "source": [
        "parameters['reload']=False\n",
        "\n",
        "if not parameters['reload']:\n",
        "    tr = time.time()\n",
        "    model.train(True)\n",
        "    for epoch in range(1,number_of_epochs):\n",
        "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
        "            count += 1\n",
        "            data = train_data[index]\n",
        "\n",
        "            ##gradient updates for each data entry\n",
        "            model.zero_grad()\n",
        "\n",
        "            sentence_in = data['words']\n",
        "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
        "            tags = data['tags']\n",
        "            chars2 = data['chars']\n",
        "\n",
        "            d = {}\n",
        "\n",
        "            ## Padding the each word to max word size of that sentence\n",
        "            chars2_length = [len(c) for c in chars2]\n",
        "            char_maxl = max(chars2_length)\n",
        "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "            for i, c in enumerate(chars2):\n",
        "                chars2_mask[i, :chars2_length[i]] = c\n",
        "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "\n",
        "            targets = torch.LongTensor(tags)\n",
        "\n",
        "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
        "            if use_gpu:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "            else:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
        "            loss += neg_log_likelihood.data / len(data['words'])\n",
        "            # loss += neg_log_likelihood.data[0] / len(data['words'])\n",
        "            neg_log_likelihood.backward()\n",
        "\n",
        "            #we use gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            #Storing loss\n",
        "            if count % plot_every == 0:\n",
        "                loss /= plot_every\n",
        "                # print(count, ': ', loss,'.\\r'))\n",
        "                # print(\"{}:{}\\r\".format(count, loss))\n",
        "                res_info = \"Count: {}, Loss: {}\".format(count, loss)\n",
        "                print(res_info)\n",
        "                # sys.stdout.write(res_info)\n",
        "                # sys.stdout.flush()\n",
        "                if losses == []:\n",
        "                    losses.append(loss)\n",
        "                losses.append(loss)\n",
        "                loss = 0.0\n",
        "\n",
        "            #Evaluating on Train, Test, Dev Sets\n",
        "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
        "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
        "                model.train(False)\n",
        "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
        "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
        "                if save:\n",
        "                    print(\"Saving Model to \", model_name)\n",
        "                    torch.save(model.state_dict(), model_name)\n",
        "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
        "\n",
        "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
        "                print()\n",
        "                model.train(True)\n",
        "\n",
        "            #Performing decay on the learning rate\n",
        "            if count % len(train_data) == 0:\n",
        "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
        "\n",
        "    print(time.time() - tr)\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "\n",
        "if not parameters['reload']:\n",
        "    #reload the best model saved from training\n",
        "    model.load_state_dict(torch.load(model_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Count: 2000, Loss: 0.4120998978614807\n",
            "Count: 4000, Loss: 0.20584198832511902\n",
            "Count: 6000, Loss: 0.19398133456707\n",
            "Count: 8000, Loss: 0.17830608785152435\n",
            "Count: 10000, Loss: 0.18000896275043488\n",
            "Count: 12000, Loss: 0.16425199806690216\n",
            "Count: 14000, Loss: 0.15671750903129578\n",
            "Count: 16000, Loss: 0.13631506264209747\n",
            "Count: 18000, Loss: 0.13559852540493011\n",
            "Count: 20000, Loss: 0.1424356997013092\n",
            "Count: 22000, Loss: 0.13580171763896942\n",
            "Count: 24000, Loss: 0.13156047463417053\n",
            "Count: 26000, Loss: 0.13369081914424896\n",
            "Count: 28000, Loss: 0.1413252204656601\n",
            "Count: 30000, Loss: 0.10883694142103195\n",
            "Count: 32000, Loss: 0.12033045291900635\n",
            "Count: 34000, Loss: 0.11910299956798553\n",
            "Count: 36000, Loss: 0.12168928980827332\n",
            "Count: 38000, Loss: 0.1026773676276207\n",
            "Count: 40000, Loss: 0.11097382009029388\n",
            "Count: 42000, Loss: 0.10947852581739426\n",
            "Count: 44000, Loss: 0.10565447062253952\n",
            "Count: 46000, Loss: 0.10718119889497757\n",
            "Count: 48000, Loss: 0.10453228652477264\n",
            "Count: 50000, Loss: 0.09872842580080032\n",
            "Count: 52000, Loss: 0.0873108059167862\n",
            "Count: 54000, Loss: 0.0901360735297203\n",
            "Count: 56000, Loss: 0.10696866363286972\n",
            "Train: new_F: 0.932096269657183 best_F: -1.0 \n",
            "Dev: new_F: 0.8880400645106528 best_F: -1.0 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8396741562975562 best_F: -1.0 \n",
            "\n",
            "Count: 58000, Loss: 0.08270511031150818\n",
            "Count: 60000, Loss: 0.08572600036859512\n",
            "Count: 62000, Loss: 0.09105607867240906\n",
            "Count: 64000, Loss: 0.09281992167234421\n",
            "Count: 66000, Loss: 0.08118379861116409\n",
            "Count: 68000, Loss: 0.0800243616104126\n",
            "Count: 70000, Loss: 0.08787484467029572\n",
            "Count: 72000, Loss: 0.08326692879199982\n",
            "Count: 74000, Loss: 0.09502013772726059\n",
            "Count: 76000, Loss: 0.08359231799840927\n",
            "Count: 78000, Loss: 0.0717618316411972\n",
            "Count: 80000, Loss: 0.079774871468544\n",
            "Count: 82000, Loss: 0.06791280955076218\n",
            "Count: 84000, Loss: 0.06894069910049438\n",
            "Count: 86000, Loss: 0.08527501672506332\n",
            "Count: 88000, Loss: 0.07747077941894531\n",
            "Count: 90000, Loss: 0.08076781779527664\n",
            "Count: 92000, Loss: 0.06679128855466843\n",
            "Count: 94000, Loss: 0.07027192413806915\n",
            "Count: 96000, Loss: 0.0742945745587349\n",
            "Count: 98000, Loss: 0.0574314147233963\n",
            "Count: 100000, Loss: 0.062144748866558075\n",
            "Count: 102000, Loss: 0.07577021420001984\n",
            "Count: 104000, Loss: 0.06747521460056305\n",
            "Count: 106000, Loss: 0.06203285604715347\n",
            "Count: 108000, Loss: 0.056689877063035965\n",
            "Count: 110000, Loss: 0.06864937394857407\n",
            "Count: 112000, Loss: 0.0684196799993515\n",
            "Train: new_F: 0.9579358473653882 best_F: 0.932096269657183 \n",
            "Dev: new_F: 0.9016266460108442 best_F: 0.8880400645106528 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8520991493643099 best_F: 0.8396741562975562 \n",
            "\n",
            "Count: 114000, Loss: 0.054135385900735855\n",
            "Count: 116000, Loss: 0.05458608642220497\n",
            "Count: 118000, Loss: 0.060316503047943115\n",
            "Count: 120000, Loss: 0.06281128525733948\n",
            "Count: 122000, Loss: 0.07320534437894821\n",
            "Count: 124000, Loss: 0.05842552334070206\n",
            "Count: 126000, Loss: 0.06367108970880508\n",
            "Count: 128000, Loss: 0.05629298463463783\n",
            "Count: 130000, Loss: 0.06292364746332169\n",
            "Count: 132000, Loss: 0.05541916936635971\n",
            "Count: 134000, Loss: 0.06612326204776764\n",
            "Count: 136000, Loss: 0.06376947462558746\n",
            "Count: 138000, Loss: 0.05518079549074173\n",
            "Count: 140000, Loss: 0.043420448899269104\n",
            "Count: 142000, Loss: 0.0448235459625721\n",
            "Count: 144000, Loss: 0.05395043268799782\n",
            "Count: 146000, Loss: 0.055414728820323944\n",
            "Count: 148000, Loss: 0.06659560650587082\n",
            "Count: 150000, Loss: 0.060197487473487854\n",
            "Count: 152000, Loss: 0.044765762984752655\n",
            "Count: 154000, Loss: 0.054929088801145554\n",
            "Count: 156000, Loss: 0.0389394611120224\n",
            "Count: 158000, Loss: 0.05499522015452385\n",
            "Count: 160000, Loss: 0.0533691830933094\n",
            "Count: 162000, Loss: 0.050800617784261703\n",
            "Count: 164000, Loss: 0.04471934214234352\n",
            "Count: 166000, Loss: 0.04147285223007202\n",
            "Count: 168000, Loss: 0.052055127918720245\n",
            "Train: new_F: 0.9690194825934206 best_F: 0.9579358473653882 \n",
            "Dev: new_F: 0.9059712902403805 best_F: 0.9016266460108442 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8502606507280245 best_F: 0.8520991493643099 \n",
            "\n",
            "Count: 170000, Loss: 0.04492691904306412\n",
            "Count: 172000, Loss: 0.04698159918189049\n",
            "Count: 174000, Loss: 0.06057768687605858\n",
            "Count: 176000, Loss: 0.046946123242378235\n",
            "Count: 178000, Loss: 0.04587112367153168\n",
            "Count: 180000, Loss: 0.04388202354311943\n",
            "Count: 182000, Loss: 0.03999923914670944\n",
            "Count: 184000, Loss: 0.04516982659697533\n",
            "Count: 186000, Loss: 0.044204745441675186\n",
            "Count: 188000, Loss: 0.05216026306152344\n",
            "Count: 190000, Loss: 0.04120773449540138\n",
            "Count: 192000, Loss: 0.03841863200068474\n",
            "Count: 194000, Loss: 0.052497874945402145\n",
            "Count: 196000, Loss: 0.04165343940258026\n",
            "Count: 198000, Loss: 0.04028391093015671\n",
            "Count: 200000, Loss: 0.04358457773923874\n",
            "Count: 202000, Loss: 0.037988729774951935\n",
            "Count: 204000, Loss: 0.04633411765098572\n",
            "Count: 206000, Loss: 0.045023754239082336\n",
            "Count: 208000, Loss: 0.04369053244590759\n",
            "Count: 210000, Loss: 0.04010919854044914\n",
            "Count: 212000, Loss: 0.049373261630535126\n",
            "Count: 214000, Loss: 0.04363406449556351\n",
            "Count: 216000, Loss: 0.04262606427073479\n",
            "Count: 218000, Loss: 0.04717482626438141\n",
            "Count: 220000, Loss: 0.03844640403985977\n",
            "Count: 222000, Loss: 0.03776265308260918\n",
            "Count: 224000, Loss: 0.03425245359539986\n",
            "Train: new_F: 0.9744234086504675 best_F: 0.9690194825934206 \n",
            "Dev: new_F: 0.9101694915254237 best_F: 0.9059712902403805 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8551687006460876 best_F: 0.8520991493643099 \n",
            "\n",
            "Count: 226000, Loss: 0.04088825359940529\n",
            "Count: 228000, Loss: 0.03678441420197487\n",
            "Count: 230000, Loss: 0.03822285309433937\n",
            "Count: 232000, Loss: 0.03740859031677246\n",
            "Count: 234000, Loss: 0.0350114107131958\n",
            "Count: 236000, Loss: 0.03287765756249428\n",
            "Count: 238000, Loss: 0.04364340007305145\n",
            "Count: 240000, Loss: 0.03606220334768295\n",
            "Count: 242000, Loss: 0.03739704191684723\n",
            "Count: 244000, Loss: 0.039379362016916275\n",
            "Count: 246000, Loss: 0.03351275622844696\n",
            "Count: 248000, Loss: 0.03730214759707451\n",
            "Count: 250000, Loss: 0.0427263081073761\n",
            "Count: 252000, Loss: 0.03740005940198898\n",
            "Count: 254000, Loss: 0.03596160560846329\n",
            "Count: 256000, Loss: 0.0395132452249527\n",
            "Count: 258000, Loss: 0.03634236007928848\n",
            "Count: 260000, Loss: 0.03378645330667496\n",
            "Count: 262000, Loss: 0.03771268576383591\n",
            "Count: 264000, Loss: 0.034299083054065704\n",
            "Count: 266000, Loss: 0.03748225048184395\n",
            "Count: 268000, Loss: 0.030509117990732193\n",
            "Count: 270000, Loss: 0.02818906493484974\n",
            "Count: 272000, Loss: 0.04362405091524124\n",
            "Count: 274000, Loss: 0.04270477965474129\n",
            "Count: 276000, Loss: 0.0319671668112278\n",
            "Count: 278000, Loss: 0.028519943356513977\n",
            "Count: 280000, Loss: 0.02897222340106964\n",
            "Count: 282000, Loss: 0.03724401444196701\n",
            "Count: 284000, Loss: 0.03237277641892433\n",
            "Count: 286000, Loss: 0.02556547336280346\n",
            "Count: 288000, Loss: 0.03389904648065567\n",
            "Count: 290000, Loss: 0.02802608534693718\n",
            "Count: 292000, Loss: 0.02777375839650631\n",
            "Count: 294000, Loss: 0.02718309871852398\n",
            "Train: new_F: 0.9806418638328683 best_F: 0.9744234086504675 \n",
            "Dev: new_F: 0.9174782162993337 best_F: 0.9101694915254237 \n",
            "Saving Model to  models/self-trained-model\n",
            "Test: new_F: 0.8632455425830393 best_F: 0.8551687006460876 \n",
            "\n",
            "Count: 296000, Loss: 0.023576928302645683\n",
            "Count: 298000, Loss: 0.02753087319433689\n",
            "Count: 300000, Loss: 0.03328167647123337\n",
            "Count: 302000, Loss: 0.03832041844725609\n",
            "Count: 304000, Loss: 0.03289121016860008\n",
            "Count: 306000, Loss: 0.03194231539964676\n",
            "Count: 308000, Loss: 0.03036239556968212\n",
            "Train: new_F: 0.980512427320938 best_F: 0.9806418638328683 \n",
            "Dev: new_F: 0.9143197481494088 best_F: 0.9174782162993337 \n",
            "Test: new_F: 0.8641975308641975 best_F: 0.8632455425830393 \n",
            "\n",
            "Count: 310000, Loss: 0.035678260028362274\n",
            "Count: 312000, Loss: 0.031236574053764343\n",
            "Count: 314000, Loss: 0.02610248699784279\n",
            "Count: 316000, Loss: 0.034111957997083664\n",
            "Count: 318000, Loss: 0.031226806342601776\n",
            "Count: 320000, Loss: 0.028197038918733597\n",
            "Count: 322000, Loss: 0.03062472864985466\n",
            "Train: new_F: 0.9818847521127361 best_F: 0.9806418638328683 \n",
            "Dev: new_F: 0.9159749280027104 best_F: 0.9174782162993337 \n",
            "Test: new_F: 0.8699245418613009 best_F: 0.8641975308641975 \n",
            "\n",
            "Count: 324000, Loss: 0.030126335099339485\n",
            "Count: 326000, Loss: 0.03083699569106102\n",
            "Count: 328000, Loss: 0.026742134243249893\n",
            "Count: 330000, Loss: 0.027145713567733765\n",
            "Count: 332000, Loss: 0.02838168852031231\n",
            "Count: 334000, Loss: 0.02506173960864544\n",
            "Count: 336000, Loss: 0.025488482788205147\n",
            "Train: new_F: 0.9820030633083731 best_F: 0.9818847521127361 \n",
            "Dev: new_F: 0.9131354140235135 best_F: 0.9174782162993337 \n",
            "Test: new_F: 0.8624362301977982 best_F: 0.8699245418613009 \n",
            "\n",
            "3290.27991938591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 480x320 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mgWXgMxDLZ5",
        "outputId": "5ebb532f-7e77-4e8b-97e8-2bf56d99be34"
      },
      "source": [
        "pickle_me('model', model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickled the obj model at /content/drive/Shareddrives/SWM/pickle2/model.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "yNP206ucqbnU",
        "outputId": "397b3228-e2c7-4aa7-bc32-e341c6ac44e3"
      },
      "source": [
        "model_testing_sentences = [\"I am at Tempe\", \"Germany representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.\"]\n",
        "\n",
        "#parameters\n",
        "lower=parameters['lower']\n",
        "\n",
        "#preprocessing\n",
        "final_test_data = []\n",
        "for sentence in model_testing_sentences:\n",
        "    s=sentence.split()\n",
        "    str_words = [w for w in s]\n",
        "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
        "    \n",
        "    # Skip characters that are not in the training set\n",
        "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
        "    \n",
        "    final_test_data.append({\n",
        "        'str_words': str_words,\n",
        "        'words': words,\n",
        "        'chars': chars,\n",
        "    })\n",
        "\n",
        "#prediction\n",
        "predictions = []\n",
        "print(\"Prediction:\")\n",
        "print(\"word : tag\")\n",
        "for data in final_test_data:\n",
        "    words = data['str_words']\n",
        "    chars2 = data['chars']\n",
        "\n",
        "    d = {} \n",
        "    \n",
        "    # Padding the each word to max word size of that sentence\n",
        "    chars2_length = [len(c) for c in chars2]\n",
        "    char_maxl = max(chars2_length)\n",
        "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "    for i, c in enumerate(chars2):\n",
        "        chars2_mask[i, :chars2_length[i]] = c\n",
        "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "    dwords = Variable(torch.LongTensor(data['words']))\n",
        "\n",
        "    # We are getting the predicted output from our model\n",
        "    if use_gpu:\n",
        "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "    else:\n",
        "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
        "\n",
        "    pred_chunks = get_chunks(predicted_id,tag_to_id)\n",
        "    temp_list_tags=['NA']*len(words)\n",
        "    for p in pred_chunks:\n",
        "        temp_list_tags[p[1]]=p[0]\n",
        "        \n",
        "    for word,tag in zip(words,temp_list_tags):\n",
        "        print(word,':',tag)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1d2a23f9cd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parameters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esf_1CJUymJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}